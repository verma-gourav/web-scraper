# Web Scraper Project

A concurrent web crawler built with TypeScript, Node.js, and JSDOM that crawls a website, extracts structured page data, and respects concurrency and page limits.

---

## ğŸ“‚ Project Structure

web-scraper/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ crawler.ts # Core crawler logic
â”‚ â”œâ”€â”€ index.ts # Entry point / CLI handling
â”‚
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md

---

### ğŸš€ Getting Started

1ï¸âƒ£ Install dependencies
pnpm install

2ï¸âƒ£ Run the crawler
pnpm start <website_url> <maxConcurrency> <maxPages>

Example:
pnpm start https://example.com 10 20

10 â†’ Maximum concurrent requests

20 â†’ Maximum unique pages to crawl

ğŸ“Š Output

The crawler returns an object like:

{
"example.com/about": {
"url": "https://example.com/about",
"h1": "About Us",
"firstParagraph": "We are a company that...",
"outgoingLinks": [...],
"imageURLs": [...]
}
}

You can easily:

- Save this to JSON

- Convert it to CSV

- Store it in a database

- Use it for SEO analysis, audits, or indexing

---

#### âš ï¸ Important Notes

- Only HTML pages are crawled

- External domains are ignored

- Page limits are enforced safely even under concurrency

- This crawler does not currently:

- Respect robots.txt

- Limit crawl depth

---

##### ğŸ§ª Running Tests

pnpm run test
